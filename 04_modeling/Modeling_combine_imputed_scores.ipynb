{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "test_file_url = '~/Documents/Data Science/Data Projects/nfl_db/sausage_factory/TESTING_combine_imputed.csv'\n",
    "\n",
    "# set random_state SEED variable\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl\n",
    "\n",
    "#svg.fonttype: path\n",
    "\n",
    "blue = '#3498DB'\n",
    "gray = '#95A5A6'\n",
    "red = '#E74C3C'\n",
    "dark_gray = '#34495E'\n",
    "green = '#2ECC71'\n",
    "purple = '#9B59B6'\n",
    "flatui = [blue, gray, red, dark_gray, green, purple]\n",
    "\n",
    "#rcParams['axes.prop_cycle'] = cycler('color', [blue, gray, red, dark_gray, green, purple])\n",
    "\n",
    "# Patches\n",
    "mpl.rc('patch', \n",
    "       linewidth=0.5, \n",
    "       facecolor=dark_gray, \n",
    "       edgecolor='w', \n",
    "       force_edgecolor=True, \n",
    "       antialiased=True)    \n",
    "  \n",
    "# Figure\n",
    "mpl.rc('figure', \n",
    "       figsize= (15, 9),\n",
    "       dpi= 200,\n",
    "       facecolor='w', \n",
    "       edgecolor='w', \n",
    "       titlesize='xx-large',\n",
    "       titleweight=700)\n",
    "\n",
    "# Grid\n",
    "mpl.rc('grid', \n",
    "       color=dark_gray,\n",
    "       alpha=0.5, \n",
    "       linewidth=0.5, \n",
    "       linestyle='-')\n",
    "\n",
    "# Axes\n",
    "mpl.rc('axes', \n",
    "       facecolor='w',\n",
    "       edgecolor=dark_gray,\n",
    "       linewidth=0.5,\n",
    "       grid=True,\n",
    "       titlesize='large',\n",
    "       labelsize='large',\n",
    "       labelcolor=dark_gray,\n",
    "       axisbelow=True)\n",
    "\n",
    "mpl.rc('axes.spines',\n",
    "       right=False,\n",
    "       top=False)\n",
    "\n",
    "# Ticks\n",
    "mpl.rc('xtick', \n",
    "       direction='out',\n",
    "       color=dark_gray)\n",
    "\n",
    "mpl.rc('xtick.major', \n",
    "       size=0.0)\n",
    "\n",
    "mpl.rc('xtick.minor', \n",
    "       size=0.0)\n",
    "\n",
    "mpl.rc('ytick', \n",
    "       direction='out',\n",
    "       color=dark_gray)\n",
    "\n",
    "mpl.rc('ytick.major', \n",
    "       size=0.0)\n",
    "\n",
    "mpl.rc('ytick.minor', \n",
    "       size=0.0)\n",
    "\n",
    "mpl.rc('legend', \n",
    "       frameon=False,\n",
    "       numpoints=1,\n",
    "       scatterpoints=1)\n",
    "\n",
    "mpl.rc('font', \n",
    "       size=13,\n",
    "       weight=400,\n",
    "       family='sans-serif')\n",
    "\n",
    "rcParams['font.sans-serif']: ['Helvetica', 'Verdana', 'Lucida Grande']\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/brad/Documents/Data Science/Data Projects/nfl_db/sausage_factory/TESTING_combine_imputed.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40e8bf288faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import data and split into features matrix and target vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/Users/brad/Documents/Data Science/Data Projects/nfl_db/sausage_factory/TESTING_combine_imputed.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# training set breakdown\n",
    "train_success = y_train.sum()\n",
    "train_total = len(y_train)\n",
    "train_percent = train_success / train_total\n",
    "print('Training Set\\nSuccesses:\\t{}\\nTotal:\\t\\t{}\\nPercent:\\t{:.3f}\\n'.format(train_success, train_total, train_percent))\n",
    "\n",
    "# test set breakdown\n",
    "test_success = y_test.sum()\n",
    "test_total = len(y_test)\n",
    "test_percent = test_success / test_total\n",
    "print('Test Set\\nSuccesses:\\t{}\\nTotal:\\t\\t{}\\nPercent:\\t{:.3f}\\n\\n'.format(test_success, test_total, test_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f0441c70a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Class 0: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Class 1: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Proportion: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtarget_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m': 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "target_count = df.success.value_counts()\n",
    "print('Class 0: {}'.format(target_count[0]))\n",
    "print('Class 1: {}'.format(target_count[1]))\n",
    "print('Proportion: {}'.format(round(target_count[0] / target_count[1]), 2), ': 1')\n",
    "\n",
    "target_count.plot(kind='bar', title = 'Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit a dummy classifier\n",
    "dummy = DummyClassifier(random_state = SEED)\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = dummy.predict(X_test)\n",
    "\n",
    "# SCORING\n",
    "# accuracy\n",
    "dummy_accuracy = dummy.score(X_test, y_test)\n",
    "print('Dummy Classifier accuracy: {:.4f}\\n\\n'.format(dummy_accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test,\n",
    "                           y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate & fit model\n",
    "model = LogisticRegression(random_state = SEED, class_weight = \"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score predictions\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print('Logistic Regression Accuracy:\\t{:.4f}\\n'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the learning algorithm\n",
    "model = LogisticRegression(random_state = SEED, class_weight = \"balanced\")\n",
    "\n",
    "# create a params dict\n",
    "penalty = ['l1', 'l2']\n",
    "C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "hyperparameters = dict(C = C, penalty = penalty)\n",
    "\n",
    "# instantiate & fit grid search\n",
    "gridsearch = GridSearchCV(model, hyperparameters, cv = 5, verbose = 0)\n",
    "best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "best_penalty = best_model.best_estimator_.get_params()['penalty']\n",
    "best_C = best_model.best_estimator_.get_params()['C']\n",
    "print('Best penalty: {}'.format(best_penalty))\n",
    "print('Best C: {}'.format(best_C))\n",
    "\n",
    "# build & fit a tuned model\n",
    "tuned_model = LogisticRegression(C = best_C, penalty = best_penalty)\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = tuned_model.predict(X_test)\n",
    "\n",
    "# score predictions\n",
    "accuracy = tuned_model.score(X_test, y_test)\n",
    "print('Tuned Accuracy:\\t{:.4f}\\n'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit the learning algorithm\n",
    "model = LinearSVC(random_state=SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit the learning algorithm\n",
    "model = LinearSVC(random_state=SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# create a params dict\n",
    "C = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "hyperparameters = dict(C = C)\n",
    "\n",
    "# instantiate & fit grid search\n",
    "gridsearch = GridSearchCV(model, hyperparameters, cv = 5, verbose = 0)\n",
    "best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "best_C = best_model.best_estimator_.get_params()['C']\n",
    "print('Best C: {}'.format(best_C))\n",
    "\n",
    "# build & fit a tuned model\n",
    "tuned_model = LinearSVC(C = best_C)\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = tuned_model.predict(X_test)\n",
    "\n",
    "# score predictions\n",
    "accuracy = tuned_model.score(X_test, y_test)\n",
    "print('Tuned Accuracy:\\t{:.4f}\\n'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the learning algorithm\n",
    "model = SVC(random_state=SEED)\n",
    "\n",
    "# create a params dict\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "\n",
    "# instantiate the learning algorithm\n",
    "model = SVC(random_state = SEED)\n",
    "\n",
    "# create a params dict\n",
    "C = [100, 150, 200, 300]\n",
    "gamma = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "hyperparameters = dict(C = C, gamma = gamma)\n",
    "\n",
    "# instantiate & fit grid search\n",
    "gridsearch = GridSearchCV(model, hyperparameters, cv = 5, verbose = 0)\n",
    "best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "best_gamma = best_model.best_estimator_.get_params()['gamma']\n",
    "best_C = best_model.best_estimator_.get_params()['C']\n",
    "print('Best gamma: {}'.format(best_gamma))\n",
    "print('Best C: {}'.format(best_C))\n",
    "\n",
    "\n",
    "# BUILD MODEL WITH BEST HYPERPARAMETERS\n",
    "\n",
    "# instantiate & fit model\n",
    "model = SVC(gamma = best_gamma, C = best_C, random_state = SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate & train a DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state = SEED)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# SCORING\n",
    "# accuracy\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the learning algorithm\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# create a params dict\n",
    "depth = [2, 3, 4, 5, 6, 10, 15, 20]\n",
    "min_samples = [.01, .025, .05, .075, .1, .2]\n",
    "hyperparameters = dict(max_depth = depth, min_samples_leaf = min_samples)\n",
    "\n",
    "# instantiate grid search\n",
    "gridsearch = GridSearchCV(model, hyperparameters, cv = 5, verbose = 0)\n",
    "\n",
    "# fit grid search\n",
    "best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "best_depth = best_model.best_estimator_.get_params()['max_depth']\n",
    "best_min_samples = best_model.best_estimator_.get_params()['min_samples_leaf']\n",
    "print('Best max_depth: {}'.format(best_depth))\n",
    "print('Best min_samples_leaf: {}'.format(best_min_samples))\n",
    "\n",
    "\n",
    "# RUN MODEL WITH BEST HYPERPARAMETERS\n",
    "\n",
    "# instantiate a DecisionTreeClassifier\n",
    "tuned_model = DecisionTreeClassifier(max_depth = best_depth,\n",
    "                                     min_samples_leaf = best_min_samples,\n",
    "                                     random_state = SEED)\n",
    "\n",
    "# train the model\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = tuned_model.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "tuned_model_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Tuned Decision Tree Accuracy:\\t{}'.format(tuned_model_accuracy))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate & fit\n",
    "model = RandomForestClassifier(random_state = SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the learning algorithm\n",
    "model = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "# create a params dict\n",
    "depth = [2, 3, 4, 5, 6, 10, 15, 20]\n",
    "min_samples = [.01, .025, .05, .075, .1, .2]\n",
    "est = [5, 10, 50, 100, 500]\n",
    "hyperparameters = dict(max_depth = depth, \n",
    "                       min_samples_leaf = min_samples,\n",
    "                       n_estimators = est)\n",
    "\n",
    "# instantiate grid search\n",
    "gridsearch = GridSearchCV(model, hyperparameters, cv = 5, verbose = 0)\n",
    "\n",
    "# fit grid search\n",
    "best_model = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "best_depth = best_model.best_estimator_.get_params()['max_depth']\n",
    "best_min_samples = best_model.best_estimator_.get_params()['min_samples_leaf']\n",
    "best_est = best_model.best_estimator_.get_params()['n_estimators']\n",
    "print('Best max_depth: {}'.format(best_depth))\n",
    "print('Best min_samples_leaf: {}'.format(best_min_samples))\n",
    "print('Best n_estimators: {}'.format(best_est))\n",
    "\n",
    "\n",
    "# BUILD MODEL WITH BEST HYPERPARAMETERS\n",
    "\n",
    "# instantiate a RandomForestClassifier\n",
    "tuned_model = RandomForestClassifier(max_depth = best_depth,\n",
    "                                     min_samples_leaf = best_min_samples,\n",
    "                                     n_estimators = best_est,\n",
    "                                     random_state = SEED)\n",
    "\n",
    "# train the model\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "tuned_model_y_pred = tuned_model.predict(X_test)\n",
    "\n",
    "# SCORING\n",
    "# accuracy\n",
    "accuracy_score(y_test, tuned_model_y_pred)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, tuned_model_y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = tuned_model_y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "\n",
    "# instantiate individual classifiers\n",
    "lr = LogisticRegression(random_state = SEED)\n",
    "\n",
    "svc = SVC(random_state = SEED)\n",
    "\n",
    "knn = KNN()\n",
    "\n",
    "rf = RandomForestClassifier(random_state = SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr),\n",
    "               ('SVM', svc),\n",
    "               ('KNN', knn),\n",
    "               ('Classification Tree', rf)]\n",
    "\n",
    "# iterate over the defined list of tuples containing the classifiers \n",
    "for clf_name, clf in classifiers:\n",
    "    # fit clf to the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # predict the labels of the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # evaluate the accuracy of clf on the test set\n",
    "    print('{:s} : {:.3f}'.format(clf_name, precision_score(y_test, y_pred)))\n",
    "    \n",
    "# instantiate a VotingClassifier 'vc'\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "\n",
    "# fit 'vc' to the training set\n",
    "vc.fit(X_train, y_train)\n",
    "\n",
    "# predict test set labels\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "# SCORING\n",
    "# accuracy\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "\n",
    "# instantiate a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# instantiate & fit a Bagging Classifier\n",
    "bc = BaggingClassifier(base_estimator=dt, \n",
    "                       n_estimators=300, \n",
    "                       n_jobs=-1)\n",
    "bc.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = bc.predict(X_test)\n",
    "\n",
    "# SCORING\n",
    "# accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}%'.format(acc * 100))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate a Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth = 2,\n",
    "                            random_state=SEED)\n",
    "\n",
    "# instantate and fit an AdaBoost Classifier\n",
    "adb_clf = AdaBoostClassifier(base_estimator = dt, \n",
    "                             n_estimators = 500,\n",
    "                             random_state = SEED)\n",
    "adb_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = adb_clf.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}%'.format(acc * 100))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "\n",
    "# drop binary features\n",
    "nb_features = X.drop(['d1'], axis = 1)\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(nb_features, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and train Gaussian Naive Bayes Classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on test features\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}%'.format(acc * 100))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import xgboost as xgb\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit an XGBoost Classifier\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: {:.4f}%'.format(acc * 100))\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - KFold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# instantiate KFoldCV\n",
    "kfold = KFold(n_splits = 10, random_state = SEED)\n",
    "\n",
    "# score and print results\n",
    "results = cross_val_score(model, X_train, y_train, cv = kfold)\n",
    "print('Accuracy: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# plot feature importance\n",
    "xgb.plot_importance(model)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Using XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit a model\n",
    "model = xgb.XGBClassifier(random_state = SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    \n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier(random_state = SEED)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    \n",
    "    # transform X_test to match selection\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    \n",
    "    # make predictions using transformed test feature matrix\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    \n",
    "    # score performance\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(\"Thresh=%.3f, n=%d, Precision: %.2f%%\" % (thresh, select_X_train.shape[1], precision*100.0));\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Feature-Selected X_train with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit a model\n",
    "model = xgb.XGBClassifier(random_state = SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision using all features: %.2f%%\" % (precision * 100.0))\n",
    "\n",
    "# select features using threshold\n",
    "selection = SelectFromModel(model, threshold=0.008, prefit=True)\n",
    "\n",
    "# transform training feature set\n",
    "select_X_train = selection.transform(X_train)\n",
    "\n",
    "print(select_X_train.shape)\n",
    "\n",
    "# instantiate the model\n",
    "selection_model = xgb.XGBClassifier(random_state = SEED)\n",
    "\n",
    "# fit the model to selected features\n",
    "selection_model.fit(select_X_train, y_train)\n",
    "    \n",
    "# select test features\n",
    "select_X_test = selection.transform(X_test)\n",
    "\n",
    "# make predictions using selected features of test set\n",
    "y_pred = selection_model.predict(select_X_test)\n",
    "\n",
    "# evaluate the model\n",
    "precision = precision_score(y_test, y_pred);\n",
    "print(\"Precision using selected features: %.2f%%\" % (precision*100.0))\n",
    "print(classification_report(y_test, y_pred));\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "xgb = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', silent=True, nthread=1)\n",
    "\n",
    "folds = 5\n",
    "param_comb = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = SEED)\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, \n",
    "                                   param_distributions = params,\n",
    "                                   n_iter = param_comb,\n",
    "                                   scoring = 'roc_auc',\n",
    "                                   n_jobs = 4,\n",
    "                                   cv = skf.split(X,y),\n",
    "                                   verbose = 3,\n",
    "                                   random_state=SEED )\n",
    "\n",
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subsample=1.0, min_child_weight=5, max_depth=5, gamma=5, colsample_bytree=0.6, score=0.7734394124847003, total=   0.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import xgboost as xgb\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "# split\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate the model\n",
    "model = xgb.XGBClassifier(gamma = 5,\n",
    "                          subsample = 1.0,\n",
    "                          min_child_weight = 5,\n",
    "                          colsample_bytree = 0.6,\n",
    "                          max_depth = 5,\n",
    "                          random_state = SEED)\n",
    "\n",
    "# fit \n",
    "model.fit(X, y)\n",
    "\n",
    "# SCORE\n",
    "#accuracy\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import xgboost as xgb\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "# split\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# instantiate and fit a model\n",
    "model = xgb.XGBClassifier(random_state = SEED)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# score accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    \n",
    "    # train model\n",
    "    selection_model = xgb.XGBClassifier(random_state = SEED)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    \n",
    "    # transform X_test to match selection\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    \n",
    "    # make predictions using transformed test feature matrix\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    \n",
    "    # score performance\n",
    "    accuracy = accuracy_score(y_test, y_pred);\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0));\n",
    "    print(classification_report(y_test,\n",
    "                               y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "# split\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# create the param grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
    "    'n_estimators': [200, 400, 800],\n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'subsample': [0.3, 0.5, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# instantiate the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# configure GridSearchCV\n",
    "grid = GridSearchCV(estimator = model, \n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 4, \n",
    "                    verbose = 1)\n",
    "\n",
    "# fit \n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "# split\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# create dmatrix\n",
    "dback_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "model = xgb.XGBClassifier(learning_rate = 0.01, \n",
    "                          n_estimators = 200,\n",
    "                          max_depth = 3,\n",
    "                          subsample = 0.3,\n",
    "                          colsample_bytree = 1.0,\n",
    "                          random_state = SEED)\n",
    "\n",
    "\n",
    "# fit \n",
    "model.fit(X, y)\n",
    "\n",
    "# SCORE\n",
    "#accuracy\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "# split\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "\n",
    "# create the param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [1000],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.5, 0.9],\n",
    "    'subsample': [0.25, 0.5, 0.75, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'gamma': [0]\n",
    "}\n",
    "\n",
    "# instantiate the model\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "# configure GridSearchCV\n",
    "grid = GridSearchCV(estimator = model, \n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'roc_auc', \n",
    "                    cv = 4, \n",
    "                    verbose = 1)\n",
    "\n",
    "# fit \n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \",grid.best_params_)\n",
    "\n",
    "# instantiate the model\n",
    "tuned_model = xgb.XGBClassifier(params = grid.best_params_, random_state = SEED)\n",
    "\n",
    "# fit \n",
    "tuned_model.fit(X, y)\n",
    "\n",
    "# SCORE\n",
    "y_pred = tuned_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "\n",
    "\n",
    "# plot feature importance\n",
    "xgb.plot_importance(tuned_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# number of features\n",
    "number_of_features = 29\n",
    "\n",
    "# create function returning a compiled network\n",
    "def create_network(optimizer = 'rmsprop'):\n",
    "    \n",
    "    # start neural network\n",
    "    network = models.Sequential()\n",
    "    \n",
    "    # add fully connected layer with a ReLU activation function\n",
    "    network.add(layers.Dense(units = 16,\n",
    "                            activation = 'relu',\n",
    "                            input_shape = (number_of_features,)))\n",
    "    \n",
    "    # add fully connected layer with a signmoid activation function\n",
    "    network.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    # compile neural network\n",
    "    network.compile(loss = 'binary_crossentropy',\n",
    "                   optimizer = optimizer,\n",
    "                   metrics = ['accuracy'])\n",
    "    \n",
    "    # return compiled network\n",
    "    return network\n",
    "\n",
    "# wrap Keras model so it can be used by sklearn\n",
    "neural_network = KerasClassifier(build_fn = create_network, verbose = 0)\n",
    "\n",
    "# create hyperparameters\n",
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "\n",
    "# create hyperparameter options\n",
    "hyperparameters = dict(optimizer = optimizers, epochs = epochs, batch_size = batches)\n",
    "\n",
    "# create grid search\n",
    "grid = GridSearchCV(estimator = neural_network, param_grid = hyperparameters)\n",
    "\n",
    "# fit grid search\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# best hyperparameters\n",
    "grid_result.best_estimator_\n",
    "grid_result.best_score_\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and split into features matrix and target vector\n",
    "df = pd.read_csv(test_file_url, index_col = 0)\n",
    "\n",
    "X, y = df[df.columns.tolist()[:-1]], df[df.columns.tolist()[-1]]\n",
    "\n",
    "# split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = SEED)\n",
    "\n",
    "# number of features\n",
    "number_of_features = 29\n",
    "\n",
    "# instantiate and fit a neural_network\n",
    "tuned_nn = KerasClassifier(build_fn = create_network, verbose = 0)\n",
    "tuned_nn.fit(X_train,\n",
    "            y_train,\n",
    "            epochs = 10,\n",
    "            verbose = 1,\n",
    "            batch_size = 5,\n",
    "            validation_data = (X_test, y_test))\n",
    "\n",
    "y_pred = tuned_nn.predict(X_test)\n",
    "\n",
    "# classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "ax.matshow(cm, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
